require({cache:{
'url:esri/views/3d/webgl-engine/materials/internal/util.xml':"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n<snippets>\n\n<snippet name=\"alignToPixelCenter\"><![CDATA[\n  vec4 alignToPixelCenter(vec4 clipCoord, vec2 widthHeight) {\n    // From clip space to (0 : 1), bias towards right pixel edge\n    vec2 xy = vec2(.500123) + .5 * clipCoord.xy / clipCoord.w;\n\n    // Size of a pixel in range (0 : 1)\n    vec2 pixelSz = vec2(1.0) / widthHeight;\n\n    // Round to nearest pixel center\n    vec2 ij = (floor(xy * widthHeight) + vec2(0.5)) * pixelSz;\n\n    // Convert back to clip space\n    vec2 result = (ij * 2.0 - vec2(1.0)) * clipCoord.w;\n\n    return vec4(result, clipCoord.zw);\n  }\n]]></snippet>\n\n<snippet name=\"alignToPixelOrigin\"><![CDATA[\n  vec4 alignToPixelOrigin(vec4 clipCoord, vec2 widthHeight) {\n    // From clip space to (0 : 1),\n    vec2 xy = vec2(.5) + .5 * clipCoord.xy / clipCoord.w;\n\n    // Size of a pixel in range (0 : 1)\n    vec2 pixelSz = vec2(1.0) / widthHeight;\n\n    // Round to nearest pixel border, (0 : 1)\n    vec2 ij = floor((xy + .5 * pixelSz) * widthHeight) * pixelSz;\n\n    // Convert back to clip space\n    vec2 result = (ij * 2.0 - vec2(1.0)) * clipCoord.w;\n\n    return vec4(result, clipCoord.zw);\n  }\n]]></snippet>\n\n<snippet name=\"float2rgba\"><![CDATA[\n\tvec4 float2rgba(const in float v) {\n\t\tvec4 enc = vec4(1.0, 255.0, 65025.0, 16581375.0) * v;\n\t\tenc = fract(enc);\n\t\tenc -= enc.yzww * vec4(1.0/255.0, 1.0/255.0, 1.0/255.0, 0.0);\n\t\treturn enc;\n\t}\n]]></snippet>\n\n<snippet name=\"rgba2float\"><![CDATA[\n\tfloat rgba2float(vec4 rgba) {\n\t\treturn dot(rgba, vec4(1.0, 1.0/255.0, 1.0/65025.0, 1.0/16581375.0));\n\t}\n]]></snippet>\n\n<snippet name=\"calcFragDepth\"><![CDATA[\n\t#extension GL_OES_standard_derivatives : enable\n\n\tfloat calcFragDepth(const in float depth) {\n\t\t//calc polygon offset\n\t\tconst float SLOPE_SCALE = 2.0;\n\t\tconst float BIAS = 2.0 * .000015259;\t\t// 1 / (2^16 - 1)\n\t\tfloat m = max(abs(dFdx(depth)), abs(dFdy(depth)));\n\t\tfloat result = depth + SLOPE_SCALE * m + BIAS;\n\t\treturn clamp(result, .0, .999999);\n\t}\n]]></snippet>\n\n<snippet name=\"evalShadow\"><![CDATA[\n\t$rgba2float\n\n\t// \"matrix\" parameter used to have const qualifier as well, but IE11 couldn't deal with it at time of writing.\n\t// once IE11 is fine with it, const should probably be re-introduced\n\tfloat evalShadow(const in vec3 vpos, const in float depth, const in sampler2D depthTex, const int num, const in vec4 distance, in mat4 matrix[4], const in float halfPxSz) {\n\t\t//choose correct cascade\n\t\tint i = depth < distance[1] ? 0 : depth < distance[2] ? 1 : depth < distance[3] ? 2 : 3;\n\n\t\tif (i >= num) return .0;\n\n\t\tmat4 mat = i == 0 ? matrix[0] : i == 1 ? matrix[1] : i == 2 ? matrix[2] : matrix[3];\n\n\t\tvec4 lv = mat * vec4(vpos, 1.0);\n\t\tlv.xy /= lv.w;\n\n\t\t//vertex completely outside? -> no shadow\n\t\tvec3 lvpos = .5 * lv.xyz + vec3(.5);\n\t\tif (lvpos.z >= 1.0) return .0;\n\t\tif (lvpos.x < .0 || lvpos.x > 1.0 || lvpos.y < .0 || lvpos.y > 1.0) return .0;\n\n\t\t//calc coord in cascade texture\n\t\tvec2 uv = vec2(float(i - 2 * (i / 2)) *.5, float(i / 2) * .5) + .5 * lvpos.xy;\n\n\t\tfloat texSize = .5 / halfPxSz;\n\n\t\t//filter, offset by half pixels\n\t\tvec2 st = fract((vec2(halfPxSz) + uv) * texSize);\n\n\t\tfloat s00 = rgba2float(texture2D(depthTex, uv + vec2(-halfPxSz, -halfPxSz))) < lvpos.z ? 1.0 : .0;\n\t\tfloat s10 = rgba2float(texture2D(depthTex, uv + vec2(halfPxSz, -halfPxSz))) < lvpos.z ? 1.0 : .0;\n\t\tfloat s11 = rgba2float(texture2D(depthTex, uv + vec2(halfPxSz, halfPxSz))) < lvpos.z ? 1.0 : .0;\n\t\tfloat s01 = rgba2float(texture2D(depthTex, uv + vec2(-halfPxSz, halfPxSz))) < lvpos.z ? 1.0 : .0;\n\n\t\treturn mix(mix(s00, s10, st.x), mix(s01, s11, st.x), st.y);\n\t}\n]]></snippet>\n\n\n<!--\n\tScene Lighting Definitions:\n\t================================================\n\n\tdefines:\n\t\t- SH_ORDER: 1|2|3\n\tinput:\n\t\t- normal: vec3\n\t\t- albedo: vec3\n\t  - shadow: float\n\t\t- ssao: float\n\treturn:\n\t  - color: vec3\n-->\n<snippet name=\"sceneLightingDefinitions\"><![CDATA[\n\t$viewingMode\n\n\t// main light\n\t/////////////////////////////////////////\n\tuniform vec3 lightingMainDirection;\n\tuniform vec3 lightingMainIntensity;\n\n\t// ambient lighting\n\t/////////////////////////////////////////\n\t#ifndef SH_ORDER\n\t\t#define SH_ORDER 2\n\t#endif\n\n\t#if SH_ORDER == 0\n\t\tuniform vec3 lightingAmbientSH0;\n\t#elif SH_ORDER == 1\n\t\tuniform vec4 lightingAmbientSH_R;\n\t\tuniform vec4 lightingAmbientSH_G;\n\t\tuniform vec4 lightingAmbientSH_B;\n\t#elif SH_ORDER == 2\n\t\tuniform vec3 lightingAmbientSH0;\n\t\tuniform vec4 lightingAmbientSH_R1;\n\t\tuniform vec4 lightingAmbientSH_G1;\n\t\tuniform vec4 lightingAmbientSH_B1;\n\t\tuniform vec4 lightingAmbientSH_R2;\n\t\tuniform vec4 lightingAmbientSH_G2;\n\t\tuniform vec4 lightingAmbientSH_B2;\n\t#endif\n\n\t// special tweaking\n\t//////////////////////////////////////////\n\t\tuniform float lightingFixedFactor;\n\t\tuniform float lightingGlobalFactor;\n\n\t\tuniform float ambientBoostFactor;\n\n\t// evaluation\n\t//////////////////////////////////////////\n\n\tvec3 evaluateSceneLighting(vec3 normal, vec3 albedo, float shadow, float ssao, vec3 additionalLight) {\n\t\t// evaluate the main light\n\t\tfloat dotVal = mix(clamp(-dot(normal, lightingMainDirection), 0.0, 1.0), 1.0, lightingFixedFactor);\n\t\tvec3 mainLight = (1.0 - shadow) * lightingMainIntensity * dotVal;\n\n\t\t// evaluate the sh ambient light\n\t\t#if SH_ORDER == 0\n\t\t\tvec3 ambientLight = 0.282095 * lightingAmbientSH0;\n\t\t#elif SH_ORDER == 1\n\t\t\tvec4 sh0 = vec4(\n\t\t\t\t0.282095,\n\t\t\t\t0.488603 * normal.x,\n\t\t\t\t0.488603 * normal.z,\n\t\t\t\t0.488603 * normal.y\n\t\t\t);\n\t\t\tvec3 ambientLight = vec3(\n\t\t\t\tdot(lightingAmbientSH_R, sh0),\n\t\t\t\tdot(lightingAmbientSH_G, sh0),\n\t\t\t\tdot(lightingAmbientSH_B, sh0)\n\t\t\t);\n\t\t#elif SH_ORDER == 2\n\t\t\tvec3 ambientLight = 0.282095 * lightingAmbientSH0;\n\n\t\t\tvec4 sh1 = vec4(\n\t\t\t\t0.488603 * normal.x,\n\t\t\t\t0.488603 * normal.z,\n\t\t\t\t0.488603 * normal.y,\n\t\t\t\t1.092548 * normal.x * normal.y\n\t\t\t);\n\t\t\tvec4 sh2 = vec4(\n\t\t\t\t1.092548 * normal.y * normal.z,\n\t\t\t\t0.315392 * (3.0 * normal.z * normal.z - 1.0),\n\t\t\t\t1.092548 * normal.x * normal.z,\n\t\t\t\t0.546274 * (normal.x * normal.x - normal.y * normal.y)\n\t\t\t);\n\t\t\tambientLight += vec3(\n\t\t\t\tdot(lightingAmbientSH_R1, sh1),\n\t\t\t\tdot(lightingAmbientSH_G1, sh1),\n\t\t\t\tdot(lightingAmbientSH_B1, sh1)\n\t\t\t);\n\t\t\tambientLight += vec3(\n\t\t\t\tdot(lightingAmbientSH_R2, sh2),\n\t\t\t\tdot(lightingAmbientSH_G2, sh2),\n\t\t\t\tdot(lightingAmbientSH_B2, sh2)\n\t\t\t);\n\t\t#endif\n\t\tambientLight *= (1.0 - ssao);\n\n\t\t// inverse gamma correction on the albedo color\n\t\tfloat gamma = 2.1;\n\t\tvec3 albedoGammaC = pow(albedo, vec3(gamma));\n\n\t\t// physically correct BRDF normalizes by PI\n\t\tconst float PI = 3.14159;\n\t\tvec3 totalLight = mainLight + ambientLight + additionalLight;\n\t\ttotalLight = min(totalLight, vec3(PI, PI, PI));\n\t\tvec3 outColor = vec3((albedoGammaC / PI) * (totalLight));\n\n\t\t// apply gamma correction to the computed color\n\t\toutColor = pow(outColor, vec3(1.0/gamma));\n\n\t\treturn outColor;\n\t}\n\n]]></snippet>\n\n<snippet name=\"sceneLightingAdditionalLightGlobal\"><![CDATA[\n\t// heuristic lighting model originally used in the terrain shading\n\t// now used to generated additional ambient light\n\t#ifdef VIEWING_MODE_GLOBAL\n\t\tfloat vndl = -dot(normalize(vpos + localOrigin), lightingMainDirection);\n\t#else\n\t\tfloat vndl = -dot(vec3(0,0,1), lightingMainDirection);\n\t#endif\n\tfloat additionalAmbientScale = smoothstep(0.0, 1.0, clamp(vndl*2.5, 0.0, 1.0));\n\tvec3 additionalLight = ssao * lightingMainIntensity * additionalAmbientScale * ambientBoostFactor * lightingGlobalFactor;\n]]></snippet>\n\n<snippet name=\"normal2envTC\"><![CDATA[\n\tvec2 normal2envTC(vec3 normal) {\n\t\tfloat v = .5 + .5 * asin(normal.y) * 0.63661977;\n\t\tfloat u = .5 - .5 * atan(normal.z, normal.x) * 0.31830988;\n\t\treturn vec2(u, v);\n\t}\n]]></snippet>\n\n<snippet name=\"vertexShaderShowDepth\"><![CDATA[\n  $vsprecisionf\n\n\tuniform mat4 proj;\n\tattribute vec2 $position;\n\tattribute vec2 $uv0;\n\tvarying vec2 vtc;\n\n\tvoid main(void) {\n\t\tgl_Position = proj * vec4($position.x, $position.y, .0, 1.0);\n\t\tvtc = $uv0;\n\t}\n]]></snippet>\n\n\t<snippet name=\"fragmentShaderShowDepth\"><![CDATA[\n\t$fsprecisionf\n\n\tuniform sampler2D depthTex;\n\tvarying vec2 vtc;\n\t$rgba2float\n\tvoid main() {\n\t//\tgl_FragColor = vec4(vec3(texture2D(depthTex, vtc).a), 1.0);\n\t\tgl_FragColor = vec4(rgba2float(texture2D(depthTex, vtc)));\n\t//\tgl_FragColor = texture2D(depthTex, vtc);\n\t}\n]]></snippet>\n\n<snippet name=\"vsUVQuad\"><![CDATA[\n  $vsprecisionf\n\n\tattribute vec2 $position;\n\tvarying vec2 uv;\n\n\tvoid main(void) {\n\t\tgl_Position = vec4($position.x, $position.y, .0, 1.0);\n\t\tuv = $position * .5 + vec2(.5);\n\t}\n]]></snippet>\n\n<snippet name=\"toScreenCoords\"><![CDATA[\n\tvec4 toScreenCoords(vec3 vertex) {\n\t\tvec4 vClipSpace = proj * view * vec4((model * vec4(vertex, 1.0)).xyz, 1.0);\n\t\tvClipSpace.xy *= screenSize;\n\t\treturn vClipSpace/abs(vClipSpace.w);\n\t}\n]]></snippet>\n\n<snippet name=\"vvUniforms\"><![CDATA[\n#if defined(VV_SIZE)\n\t#define VV_CUSTOM_MODEL_MATRIX\n#endif\n\n#if defined(VV_SIZE)\n\tuniform vec3 vvSizeMinSize;\n\tuniform vec3 vvSizeMaxSize;\n\tuniform vec3 vvSizeOffset;\n\tuniform vec3 vvSizeFactor;\n#elif defined(VV_CUSTOM_MODEL_MATRIX)\n\tuniform vec3 vvSizeValue;\n#endif\n\n#ifdef VV_CUSTOM_MODEL_MATRIX\n\tuniform mat3 vvSymbolRotation;\n#endif\n\n#ifdef VV_CUSTOM_MODEL_MATRIX\n\tuniform vec3 vvSymbolAnchor;\n#endif\n\n#ifdef VV_COLOR\n\t#define VV_COLOR_N 8\n\tuniform float vvColorValues[VV_COLOR_N];\n\tuniform vec4 vvColorColors[VV_COLOR_N];\n#endif\n\n]]></snippet>\n\n<snippet name=\"vvFunctions\"><![CDATA[\n// Evaluation of size\n#if defined(VV_SIZE)\n\tvec3 vvGetScale(vec4 featureAttribute) {\n\t\treturn clamp(vvSizeOffset + featureAttribute.x * vvSizeFactor, vvSizeMinSize, vvSizeMaxSize);\n\t}\n#elif defined(VV_CUSTOM_MODEL_MATRIX)\n\tvec3 vvGetScale(vec4 featureAttribute) {\n\t\treturn vvSizeValue;\n\t}\n#endif\n\n// Applying the model matrix\n#ifdef VV_CUSTOM_MODEL_MATRIX\n\tvec4 vvTransformPosition(vec3 position, vec4 featureAttribute) {\n\t\treturn vec4(vvSymbolRotation * (vvGetScale(featureAttribute) * (position + vvSymbolAnchor)), 1.0);\n\t}\n\n\tvec4 vvTransformNormal(vec3 normal, vec4 featureAttribute) {\n\t\t// Normal transform is the inverse transpose of model transform\n\t\treturn vec4(vvSymbolRotation * normal / vvGetScale(featureAttribute), 1.0);\n\t}\n#endif\n\n#ifdef VV_COLOR\n\tvec4 vvGetColor(vec4 featureAttribute, float values[VV_COLOR_N], vec4 colors[VV_COLOR_N]) {\n\t\tfloat value = featureAttribute.y;\n\t\tif (value <= values[0]) {\n\t\t\treturn colors[0];\n\t\t}\n\n\t\tfor (int i = 1; i < VV_COLOR_N; ++i) {\n\t\t\tif (values[i] >= value) {\n\t\t\t\tfloat f = (value - values[i-1]) / (values[i] - values[i-1]);\n\t\t\t\treturn mix(colors[i-1], colors[i], f);\n\t\t\t}\n\t\t}\n\n\t\treturn colors[VV_COLOR_N - 1];\n\t}\n#endif\n]]></snippet>\n\n<snippet name=\"rgb2hsv\"><![CDATA[\nvec3 rgb2hsv(vec3 c)\n{\n\tvec4 K = vec4(0.0, -1.0 / 3.0, 2.0 / 3.0, -1.0);\n\tvec4 p = mix(vec4(c.bg, K.wz), vec4(c.gb, K.xy), step(c.b, c.g));\n\tvec4 q = mix(vec4(p.xyw, c.r), vec4(c.r, p.yzx), step(p.x, c.r));\n\n\tfloat d = q.x - min(q.w, q.y);\n\tfloat e = 1.0e-10;\n\treturn vec3(abs(q.z + (q.w - q.y) / (6.0 * d + e)), d / (q.x + e), q.x);\n}\n]]></snippet>\n\n<snippet name=\"hsv2rgb\"><![CDATA[\nvec3 hsv2rgb(vec3 c)\n{\n\tvec4 K = vec4(1.0, 2.0 / 3.0, 1.0 / 3.0, 3.0);\n\tvec3 p = abs(fract(c.xxx + K.xyz) * 6.0 - K.www);\n\treturn c.z * mix(K.xxx, clamp(p - K.xxx, 0.0, 1.0), c.y);\n}\n]]></snippet>\n\n<snippet name=\"colorMixMode\"><![CDATA[\n$rgb2hsv\n$hsv2rgb\n\n\n/*\n * The color mix modes are encoded in the symbol color as follows:\n *  - Fully transparent symbols are represented with alpha 0 for\n *    all color mix modes (except ignore).\n *  - color mix mode ignore is encoded as multiply with white\n *  - the other 3 color mix modes (tint, replace, multiply) are\n *    equally distributed on the remaining 255 alpha values, which\n *    gives us 85 possible alpha values\n *\n * alpha             0 : fully transparent\n * alpha in [  1 -  85]: tint\n * alpha in [ 86 - 170]: replace\n * alpha in [171 - 255]: multiply\n */\nvec4 decodeSymbolColor(vec4 symbolColor, out int colorMixMode) {\n  float symbolAlpha = 0.0;\n\n  const float maxTint = 85.0;\n  const float maxReplace = 170.0;\n  const float scaleAlpha = 3.0;\n\n  if (symbolColor.a == 0.0) {\n    colorMixMode = 1; // fully transparent -> multiply\n    symbolAlpha = 0.0;\n  }\n  else if (symbolColor.a <= maxTint) {\n    colorMixMode = 0; // tint\n    symbolAlpha = scaleAlpha * symbolColor.a;\n  }\n  else if (symbolColor.a <= maxReplace) {\n    colorMixMode = 3; // replace\n    symbolAlpha = scaleAlpha * (symbolColor.a - maxTint);\n  }\n  else {\n    colorMixMode = 1;  // multiply\n    symbolAlpha = scaleAlpha * (symbolColor.a - maxReplace);\n  }\n\n  return vec4(symbolColor.rgb, symbolAlpha);\n}\n\nvec3 mixExternalColor(vec3 internalColor, vec3 textureColor, vec3 externalColor, int mode) {\n  if (mode == 1 /* multiply */) {\n    return internalColor * textureColor * externalColor;\n  }\n  else if (mode == 2 /* ignore */ ) {\n    return internalColor * textureColor;\n  }\n  else if (mode == 3 /* replace */ ) {\n    return externalColor;\n  }\n  else {\n    // tint (or something invalid)\n    vec3 hsvIn = rgb2hsv(internalColor * textureColor);\n    vec3 hsvTint = rgb2hsv(externalColor);\n    vec3 hsvOut = vec3(hsvTint.x, hsvTint.y, hsvIn.z * hsvTint.z);\n    return hsv2rgb(hsvOut);\n  }\n}\n\nfloat mixExternalOpacity(float internalOpacity, float textureOpacity, float externalOpacity, int mode) {\n  if (mode == 2 /* ignore */ ) {\n    return internalOpacity * textureOpacity;\n  }\n  else if (mode == 3 /* replace */ ) {\n    return externalOpacity;\n  }\n  else {\n    // multiply or tint (or something invalid)\n    return internalOpacity * textureOpacity * externalOpacity;\n  }\n}\n\n]]></snippet>\n\n<snippet name=\"highlightWrite\"><![CDATA[\n  // the following uniforms are common to all highlight shaders:\n  // uniform sampler2D depthTex\n  // uniform vec4 highlightViewportPixelSz\n  float sceneDepth = texture2D(depthTex, (gl_FragCoord.xy - highlightViewportPixelSz.xy) * highlightViewportPixelSz.zw).r;\n  if (gl_FragCoord.z > sceneDepth + 5e-6) {\n    gl_FragColor = vec4(1.0, 1.0, 0.0, 1.0);\n  }\n  else {\n    gl_FragColor = vec4(1.0, 0.0, 1.0, 1.0);\n  }\n]]></snippet>\n\n<snippet name=\"screenSizePerspective\"><![CDATA[\n#ifdef SCREEN_SIZE_PERSPECTIVE\n\n// These could be functions, but I wanted to make sure that they are inlined. Additionally,\n// as macros, applying the scale works also for different types (float, vec2, etc).\n// Note that the implementation here should be kept in sync with the corresponding\n// CPU implementation (used for hitTest etc) in screenSizePerspectiveUtils.ts\n\n// Transform the input minimum size (factor.z) so that, when comparing to it, we are\n// excluding the 'padding' size (factor.w).\n#define screenSizePerspectiveMinSize(/* float */ size, /* vec4 */ factor) (factor.z * (1.0 + 2.0 * factor.w  / size))\n\n#define screenSizePerspectiveFactor(/* float */ absCosAngle) (absCosAngle * absCosAngle * absCosAngle)\n\n#define screenSizePerspectiveScaleFactor(/* float */ absCosAngle, /* float */ distanceToCamera, /* vec4 */ params) vec4(min(params.x / (distanceToCamera - params.y), 1.0), screenSizePerspectiveFactor(absCosAngle), params.z, params.w)\n\n// Factor is computed from screenSizePerspectiveScaleFactor\n#define applyScreenSizePerspectiveScaleFactorFloat(/* float */ size, /* vec4 */ factor) max(mix(size * factor.x, size, factor.y), screenSizePerspectiveMinSize(size, factor))\n#define screenSizePerspectiveScaleFloat(/* float */ size, /* float */ absCosAngle, /* float */ distanceToCamera, /* vec4 */ params) applyScreenSizePerspectiveScaleFactorFloat(size, screenSizePerspectiveScaleFactor(absCosAngle, distanceToCamera, params))\n\n#define applyScreenSizePerspectiveScaleFactorVec2(/* vec2 */ size, /* vec4 */ factor) mix(size * clamp(factor.x, screenSizePerspectiveMinSize(size.y, factor) / size.y, 1.0), size, factor.y)\n#define screenSizePerspectiveScaleVec2(/* vec2 */ size, /* float */ absCosAngle, /* float */ distanceToCamera, /* vec4 */ params) applyScreenSizePerspectiveScaleFactorVec2(size, screenSizePerspectiveScaleFactor(absCosAngle, distanceToCamera, params))\n\n#endif\n]]></snippet>\n\n<snippet name=\"selectShadingNormal\"><![CDATA[\n\t#ifdef GROUND_NORMAL_SHADING\n\t\t#ifdef VIEWING_MODE_GLOBAL\n\t\t\tvec3 shadingNormal = normalize(vpos + localOrigin);\n\t\t#else\n\t\t\tvec3 shadingNormal = vec3(0,0,1);\n\t\t#endif\n\t#else\n\t\tvec3 shadingNormal = normal;\n\t#endif\n]]></snippet>\n\n</snippets>\n",
'url:esri/views/3d/webgl-engine/materials/internal/hud.xml':"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n<snippets>\n\n<snippet name=\"commonAttributesAndUniformsHUD\"><![CDATA[\n  attribute vec3 $position;\n  attribute vec3 $normal;\n  attribute vec4 $auxpos1;\n\n  uniform mat4 proj;\n\n  uniform mat4 view;\n  uniform mat4 viewNormal;\n\n  uniform mat4 model;\n  uniform mat4 modelNormal;\n\n  uniform vec4 viewport;\n\n  uniform vec3 camPos;\n\n  uniform float polygonOffset;\n  uniform float cameraGroundRelative;\n\n#ifdef VERTICAL_OFFSET\n\n  // [ screenLength, distanceFactor, minWorldLength, maxWorldLength ]\n  uniform vec4 verticalOffset;\n\n#endif\n\n#ifdef SCREEN_SIZE_PERSPECTIVE\n\n  // [ divisor, offset, minPixelSize, paddingPixels ]\n  uniform vec4 screenSizePerspectiveAlignment;\n\n#endif\n\n  uniform sampler2D hudVisibilityTexture;\n]]></snippet>\n\n<snippet name=\"projectPositionHUD\"><![CDATA[\n  $screenSizePerspective\n\n  // Corresponds to cos(10 deg), used to compare against dot product of two vectors\n  const float SMALL_OFFSET_ANGLE = 0.984807753012208;\n\n  struct ProjectHUDAux {\n    vec3 posModel;\n    vec3 posView;\n    vec3 vnormal;\n\n    float distanceToCamera;\n    float absCosAngle;\n  };\n\n\n  /**\n   * Apply the simulated polygon offset for HUD objects that improves\n   * issues with Z-fighting.\n   *\n   * @param posView {vec3} (inout) the position in view space. Will be modified in place.\n   * @param pointGroundDistance {float} the distance from the point geometry to the ground surface.\n   * @param absCosAngle {float} the absolute cosine of the angle between the world-up at the point geometry\n   *   and the view direction.\n   *\n   * Dependencies:\n   *\n   *   Attributes:\n   *     - auxpos1: contains centerOffset and pointGroundDistance\n   *\n   *   Uniforms:\n   *     - cameraGroundRelative: indicates whether camera is above (1) or below (-1) ground.\n   *         This is used for emulated polygon offset for improved visibility of points sitting on the surface.\n   *     - polygonOffset: a constant polygon offset to bring the point closer to the viewer for\n   *         reduced flickering.\n   *     - viewport: the viewport [x, y, width, height]\n   */\n  float applyHUDViewDependentPolygonOffset(float pointGroundDistance, float absCosAngle, inout vec3 posView) {\n    float pointGroundSign = sign(pointGroundDistance);\n\n    if (pointGroundSign == 0.0) {\n      pointGroundSign = 1.0;\n    }\n\n    // cameraGroundRelative is -1 if camera is below ground, 1 if above ground\n    // groundRelative is 1 if both camera and symbol are on the same side of the ground, -1 otherwise\n    float groundRelative = cameraGroundRelative * pointGroundSign;\n\n    // view angle dependent part of polygon offset emulation\n    // we take the absolute value because the sign that is dropped is\n    // instead introduced using the ground-relative position of the symbol and the camera\n    if (polygonOffset > .0) {\n      float cosAlpha = clamp(absCosAngle, 0.01, 1.0);\n\n      float tanAlpha = sqrt(1.0 - cosAlpha * cosAlpha) / cosAlpha;\n      float factor = (1.0 - tanAlpha / viewport[2]);\n\n      // same side of the terrain\n      if (groundRelative > 0.0) {\n        posView *= factor;\n      }\n      // opposite sides of the terrain\n      else {\n        posView /= factor;\n      }\n    }\n\n    return groundRelative;\n  }\n\n  /**\n   * Project the 3d position of a HUD object from world space to clip space. In addition\n   * to standard model view projection, it also emulates a polygon offset to\n   * help with points above/below ground and icon flickering. The resulting location\n   * is the anchor of the HUD object, i.e. the position that is used also for testing\n   * visibility of the HUD object. Note that the returned projected position is not\n   * aligned to a pixel center or border, it is up to the caller to align if necessary.\n   *\n   * Dependencies:\n   *\n   *   Attributes:\n   *     - position: contains the point world position\n   *     - normal: contains the world normal pointing up at the point\n   *     - auxpos1: contains centerOffset and pointGroundDistance\n   *\n   *   Uniforms:\n   *     - model: the object -> world transformation matrix\n   *     - modelNormal: the object -> world normal transformation matrix (inv transp of model)\n   *     - view: the world -> view transformation matrix\n   *     - viewNormal: the world -> view normal transformation matrix (inv transp of view)\n   *     - proj: the view -> clip projection matrix\n   *     - verticalOffset: a vec4 containing:\n   *         - the screen height of the vertical offset\n   *         - the screen height of the vertical offset as a fraction of camera distance.\n   *         - the minimum world size vertical offset.\n   *         - the maximum world size vertical offset.\n   *       This will do a screen sized offset of the point along its normal (used for line callouts)\n   *     - screenSizePerspectiveAlignment: a vec3 containing\n   *         - the view distance dependent divisor\n   *         - the view distance dependent offset\n   *         - the minimum pixel size\n   *         - the amount of padding in pixels around the region to be scaled (not used for alignment)\n   *     - cameraGroundRelative: indicates whether camera is above (1) or below (-1) ground.\n   *         This is used for emulated polygon offset for improved visibility of points sitting on the surface.\n   *     - polygonOffset: a constant polygon offset to bring the point closer to the viewer for\n   *         reduced flickering.\n   *     - camPos: the position of the camera in world space\n   *     - viewport: the viewport [x, y, width, height]\n   */\n  vec4 projectPositionHUD(out ProjectHUDAux aux) {\n    // centerOffset is in view space and is used to implement world size offsetting\n    // of labels with respect to objects. It also pulls the label towards the viewer\n    // so that the label is visible in front of the object.\n    vec3 centerOffset = $auxpos1.xyz;\n\n    // The pointGroundDistance is the distance of the geometry to the ground and is\n    // negative if the point is below the ground, or positive if the point is above\n    // ground.\n    float pointGroundDistance = $auxpos1.w;\n\n    aux.posModel = (model * vec4($position, 1.0)).xyz;\n    aux.posView = (view * vec4(aux.posModel, 1.0)).xyz;\n    aux.vnormal = (modelNormal * vec4($normal, 1.0)).xyz;\n\n    // Screen sized offset in world space, used for example for line callouts\n    // Note: keep this implementation in sync with the CPU implementation, see\n    //   - MaterialUtil.verticalOffsetAtDistance\n    //   - HUDMaterial.applyVerticalOffsetTransformation\n\n    aux.distanceToCamera = length(aux.posView);\n\n    vec3 viewDirObjSpace = normalize(camPos - aux.posModel);\n    float cosAngle = dot(aux.vnormal, viewDirObjSpace);\n\n    aux.absCosAngle = abs(cosAngle);\n\n#ifdef SCREEN_SIZE_PERSPECTIVE\n\n#if defined(VERTICAL_OFFSET) || defined(CENTER_OFFSET_UNITS_SCREEN)\n    vec4 perspectiveFactor = screenSizePerspectiveScaleFactor(aux.absCosAngle, aux.distanceToCamera, screenSizePerspectiveAlignment);\n#endif\n\n#endif\n\n#ifdef VERTICAL_OFFSET\n\n#ifdef SCREEN_SIZE_PERSPECTIVE\n    float verticalOffsetScreenHeight = applyScreenSizePerspectiveScaleFactorFloat(verticalOffset.x, perspectiveFactor);\n#else\n    float verticalOffsetScreenHeight = verticalOffset.x;\n#endif\n\n    float worldOffset = clamp(verticalOffsetScreenHeight * verticalOffset.y * aux.distanceToCamera, verticalOffset.z, verticalOffset.w);\n    vec3 modelOffset = aux.vnormal * worldOffset;\n\n    aux.posModel += modelOffset;\n\n    vec3 viewOffset = (viewNormal * vec4(modelOffset, 1.0)).xyz;\n    aux.posView += viewOffset;\n\n    // Since we elevate the object, we need to take that into account\n    // in the distance to ground\n    pointGroundDistance += worldOffset;\n\n#endif\n\n    float groundRelative = applyHUDViewDependentPolygonOffset(pointGroundDistance, aux.absCosAngle, aux.posView);\n\n#ifndef CENTER_OFFSET_UNITS_SCREEN\n    // Apply x/y in view space, but z in screen space (i.e. along posView direction)\n    aux.posView += vec3(centerOffset.x, centerOffset.y, 0);\n\n    // Same material all have same z != 0.0 condition so should not lead to\n    // branch fragmentation and will save a normalization if it's not needed\n    if (centerOffset.z != 0.0) {\n      aux.posView -= normalize(aux.posView) * centerOffset.z;\n    }\n#endif\n\n    vec4 posProj = proj * vec4(aux.posView, 1.0);\n\n#ifdef CENTER_OFFSET_UNITS_SCREEN\n\n#ifdef SCREEN_SIZE_PERSPECTIVE\n    float centerOffsetY = applyScreenSizePerspectiveScaleFactorFloat(centerOffset.y, perspectiveFactor);\n#else\n    float centerOffsetY = centerOffset.y;\n#endif\n\n    posProj.xy += vec2(centerOffset.x, centerOffsetY) * 2.0 / viewport.zw * posProj.w;\n\n#endif\n\n    // constant part of polygon offset emulation\n    posProj.z -= groundRelative * polygonOffset * posProj.w;\n\n    return posProj;\n  }\n\n  /**\n   * Test for visibility of a HUD object.\n   *\n   * Dependencies:\n   *\n   *   Uniforms:\n   *     - hudVisibilityTexture: the texture that contains the visibility information\n   *     - markerColor: the special marker color that is used to write visibility information\n   *     - viewport: the viewport\n   */\n  bool testVisibilityHUD(vec4 posProj) {\n    // For occlusion testing, use the nearest pixel center to avoid\n    // subpixel filtering messing up the color we use to test for\n    vec4 posProjCenter = alignToPixelCenter(posProj, viewport.zw);\n\n    return texture2D(hudVisibilityTexture, .5 + .5 * posProjCenter.xy / posProjCenter.w).r > 0.0;\n  }\n]]></snippet>\n\n</snippets>\n"}});
// COPYRIGHT Â© 2017 Esri
//
// All rights reserved under the copyright laws of the United States
// and applicable international laws, treaties, and conventions.
//
// This material is licensed for use under the Esri Master License
// Agreement (MLA), and is bound by the terms of that agreement.
// You may redistribute and use this code without modification,
// provided you adhere to the terms of the MLA and include this
// copyright notice.
//
// See use restrictions at http://www.esri.com/legal/pdfs/mla_e204_e300/english
//
// For additional information, contact:
// Environmental Systems Research Institute, Inc.
// Attn: Contracts and Legal Services Department
// 380 New York Street
// Redlands, California, USA 92373
// USA
//
// email: contracts@esri.com
//
// See http://js.arcgis.com/4.4/esri/copyright.txt for details.

define(["dojo/text!./internal/util.xml","dojo/text!./internal/hud.xml","./BillboardMaterial","./ColorMaterial","./HUDMaterial","./LineCalloutMaterial","./LeafCardMaterial","./Material","./RibbonLineMaterial","./WaterMaterial","./internal/SimpleGLMaterial","./internal/TexOnlyGLMaterial","./internal/BlendLayers"],function(a,e,l,r,d,i,t,n,o,s,h,S,M){return{initializeShaders:function(u,L,x,f){u._parse(a),u._parse(e),h.loadShaders(u,L,x,f),S.loadShaders(u,L,x,f),n.loadShaders(u,L,x,f),l.loadShaders(u,L,x,f),d.loadShaders(u,L,x,f),i.loadShaders(u,L,x,f),t.loadShaders(u,L,x,f),o.loadShaders(u,L,x,f),s.loadShaders(u,L,x,f),M.loadShaders(u,L,x,f),r.loadShaders(u,L,x,f)}}});